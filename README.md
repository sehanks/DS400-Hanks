# DS400-Hanks
Data Science Capstone Project

### Dataset: https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP2/E8H2MF 

### Commit 1: Data Collection and Preprocessing
In this commit I imported all of the necessary libraries, read in the dataset, and completed basic preprocessing. For this project, I am using the Toronto Emotional Speech Set (TESS) from the Northwestern University Auditory Test No. 6. This dataset was particularly appealing because it solely includes females and yet the audio is of such good caliber. Due to other datasets' vast amount of male speakers, there is an imbalance in representation. Two women (26 and 64 years old) recited a set of 200 keywords in the sentence "Say the word _," and recordings evoking each of the following emotions were made (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are a total of 2800 audio files. Each of the two females and their emotions is contained within their own folder in the dataset. The 200 target words audio files (in WAV format) are contained within those.


After collecting the data, I was able to read in the folders with all of the audio files in them, then create a dataframe with each of the file's designated emotion and path. After doing a few basic preprocessing steps I noticed that there was an emotion with no path and after looking into the issue, decided to drop that row.

### Commit 2: Exploratory Data Analysis, Data Augmentation, and Feature Extraction
In this commit, I completed Exploratory Data Analysis (EDA), Data Augmentation, and Feature Extraction. For EDA, I created four different types of visualizations. The first visualization was a count plot of each of the different emotions. This plot once again ensured that there is an equal number of emotions. The next visualization I created was a waveplot, which shows how loud the audio file is at different times. I then created a spectrogram, which is a graphic of the frequency spectrum of sound or other signals as it varies over time. The last visualization I created was a Mel-Frequency Cepstral Coeffecients (MFCC) graph. This graph is specifically used in sound processing and is a representation of the short-term power spectrum of a sound. For each of the seven emotions, I printed out a waveplot, spectrogram, and MFCC graph in order to compare what the different emotions looked like when the same sentence was spoken. In order to get an even better understanding of the emotions, I printed the MFCC graphs side-by-side. This allowed me to see a closer comparison of each emotion in a graph. 


Data Augmentation is the technique of creating copies of a dataset and artifically altering that dataset. By adding minor changes to my dataset, I can generate new data samples with this technique. I can use alterations such as noise injection, time shifting, pitch and speed changes, etc. to produce data for the audio. Making my models resistant to these disturbances will increase their transferability into real-world situations. The label from the training set must be preserved when adding the disturbances for this to work. First I needed to determine which augmentation strategies fit my dataset the best. After attempting many different alterations, I decided to use noise injection, pitch changes, speed changes, and time shifting. 


A crucial step in studying and discovering relationships between various variables is feature extraction. I need to transform the audio data presented into a type that the models can interpret. Since the models cannot simply interpret the information supplied by the audio files, I must turn it into a structure that can be comprehended by them via feature extraction. I can conduct a number of transformations on the data and sampling rate to derive useful information from them. First I created a function in order to extract the features. I decided to use MFCC, Zero Crossing Rate, Mel Spectrogram, Root Mean Squared Value, Chroma Stft, and Spectral Centroid. MFCC forms a cepstral representation, Zero Crossing Rate computes the zero-crossing rate of an audio time series, Mel Spectrogram computes a mel-scaled spectrogram, Root Mean Squared Value computes root-mean-square (RMS) value for each frame, Chroma Stft computes a chromagram from a waveform or power spectrogram, and Spectral Centroid computes the spectral centroid. I then created a function to get those extracted features and perform the data augmentation transformations on them. I will have then applied data augmentation and extracted the features for each audio files and saved them.


